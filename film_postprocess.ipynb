{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e4c835-1ac2-485b-85a4-3d2180b2dcf9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0e1f3b-3ffc-492e-8b98-8f09c678825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cell\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import gzip\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57ed2-187a-41a7-9f52-8b9c4a60a123",
   "metadata": {},
   "source": [
    "### Extract data from HTML from sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408eba0c-3580-4d74-b864-c3060d367a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from db import scan_table_limit_offset, decompress_string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "db_conn = sqlite3.connect(os.path.join(os.path.dirname(__name__), \"data/films.db\"))\n",
    "\n",
    "def get_film_id_from_url(url):\n",
    "    film_id = url.split('/')[4]\n",
    "    assert film_id[0:2] == 'tt'\n",
    "    return film_id\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb3a45-f674-44e6-8334-8b122135855b",
   "metadata": {},
   "source": [
    "### Create logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc62b70-a4a3-4fd5-b25e-83295f2b3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger():\n",
    "    # Create logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_folder = os.path.join(os.path.dirname(__name__), \"logs\")\n",
    "    if not os.path.isdir(log_folder):\n",
    "        os.mkdir(log_folder)\n",
    "        \n",
    "    log_file = os.path.join(log_folder, f\"postprocess_log_{current_time}.txt\")\n",
    "\n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatter and add it to the handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the file handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61111423-a709-4cf3-a105-01cc173ea32a",
   "metadata": {},
   "source": [
    "### Generate keyword TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f56da4-d403-4e2f-9e01-94b8cbef9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_keywords(df):\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"ipc-metadata-list-summary-item__tc\")\n",
    "        keywords = [p.text.strip() for p in items]\n",
    "        if len(keywords) == 0:\n",
    "            logger.debug('No keywords found '+film_id)\n",
    "            if \"have any plot keywords for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                keywords.append('None')\n",
    "        keywords_df = pd.DataFrame(keywords, columns=['keyword'])\n",
    "        keywords_df['film_id'] = film_id\n",
    "        \n",
    "        all_dfs.append(keywords_df)\n",
    "\n",
    "logger.info(\">>> Scraping Keywords \")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%keywords%'\", 10000, process_keywords)\n",
    "all_keywords_df = pd.concat(all_dfs)\n",
    "all_keywords_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_keywords.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc2484-e931-4def-a7bc-63d90e46b753",
   "metadata": {},
   "source": [
    "### Generate plot summaries TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6022ff-7763-43d1-8847-9b40f3913ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs_summary = []\n",
    "\n",
    "# scan pages\n",
    "def process_plots(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"_counter\n",
    "        sectionA = soup.find_all('section', class_='ipc-page-section ipc-page-section--base')\n",
    "        for sectB in sectionA:\n",
    "            containerA = sectB.find('div', class_=\"ipc-title ipc-title--base ipc-title--section-title ipc-title--on-textPrimary\")\n",
    "\n",
    "            # check if page is valid\n",
    "            if containerA:\n",
    "                titleA = containerA.find('span').get_text()\n",
    "\n",
    "                # Extract Summaries\n",
    "                if titleA == 'Summaries':\n",
    "                    boxes = sectB.find_all(class_=\"ipc-html-content ipc-html-content--base\")\n",
    "\n",
    "                    # Check if summaries section has valid data\n",
    "                    if boxes:\n",
    "                        item_len = 0\n",
    "                        for box in boxes:\n",
    "                            item_len += 1\n",
    "                            items = box.find_all(class_=\"ipc-html-content-inner-div\")\n",
    "                            item_texts = [p.text.strip() for p in items]\n",
    "                            item_texts = [clean_summary(p) for p in item_texts]\n",
    "                            if len(item_texts) == 0:\n",
    "                                logger.debug('No summaries found '+film_id)\n",
    "                                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                                    logger.debug('Page content not valid '+film_id)\n",
    "                                    item_texts.append('None')\n",
    "                            item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                            item_df['film_id'] = film_id\n",
    "                            item_df['summary_id'] = item_len\n",
    "                            all_dfs_summary.append(item_df)\n",
    "                            \n",
    "                    else:    # If summaries section is not valid enter empty row for the film\n",
    "                        logger.debug('No summaries found '+film_id)\n",
    "                        if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                            logger.debug('Page content not valid '+film_id)\n",
    "                        item_texts=['None']\n",
    "                        item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                        item_df['film_id'] = film_id\n",
    "                        item_df['summary_id'] = 1\n",
    "                        all_dfs_summary.append(item_df)                       \n",
    "            \n",
    "            else:\n",
    "                # if page is not valid, enter empty row for summary and synopsis for the film\n",
    "                logger.debug('No plots found '+film_id)\n",
    "                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                    logger.debug('Page content not valid '+film_id)\n",
    "                item_texts=['None']\n",
    "                item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                item_df['film_id'] = film_id\n",
    "                item_df['summary_id'] = 1\n",
    "                all_dfs_summary.append(item_df)\n",
    "\n",
    "\n",
    "logger.info(\">>> Scraping Plot Summary\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%plotsummary%'\", 10000, process_plots)\n",
    "all_plots_df = pd.concat(all_dfs_summary)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_plots.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688f419-cfd5-4b76-9b3f-a6b585eefad7",
   "metadata": {},
   "source": [
    "### Generate synopsis TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079dd357-c66b-4628-be4a-8256413f2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs_synopsis = []\n",
    "\n",
    "# scan pages\n",
    "def process_synopsis(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"_counter\n",
    "        sectionA = soup.find_all('section', class_='ipc-page-section ipc-page-section--base')\n",
    "        for sectB in sectionA:\n",
    "            containerA = sectB.find('div', class_=\"ipc-title ipc-title--base ipc-title--section-title ipc-title--on-textPrimary\")\n",
    "\n",
    "            # check if page is valid\n",
    "            if containerA:\n",
    "                titA = containerA.find('span').get_text()\n",
    "\n",
    "                if titA == 'Synopsis':\n",
    "                    boxes = sectB.find_all(class_=\"ipc-html-content ipc-html-content--base\")\n",
    "                    # Check if synopsis section has valid data\n",
    "                    if boxes:\n",
    "                        for box in boxes:\n",
    "                            items = box.find_all(class_=\"ipc-html-content-inner-div\")\n",
    "                            item_texts = [p.text.strip() for p in items]\n",
    "                            item_texts = [clean_summary(p) for p in item_texts]\n",
    "                            if len(item_texts) == 0:\n",
    "                                logger.debug('No synopsis found '+film_id)\n",
    "                                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                                    #logger.debug('Page content not valid '+film_id)\n",
    "                                    item_texts.append('None')\n",
    "                            item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                            item_df['film_id'] = film_id\n",
    "                            all_dfs_synopsis.append(item_df)\n",
    "                    else:\n",
    "                        # If synopsis section is not valid enter empty row for the film\n",
    "                        logger.debug('No synopsis found '+film_id)\n",
    "                        if \"have any synopsis for this title yet\" in html.unescape(html_content):\n",
    "                            logger.debug('Page content not valid '+film_id)\n",
    "                        item_texts=['None']\n",
    "                        item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                        item_df['film_id'] = film_id\n",
    "                        all_dfs_synopsis.append(item_df)\n",
    "                        \n",
    "            else:\n",
    "                # if page is not valid, enter empty row for summary and synopsis for the film\n",
    "                logger.debug('No plots found '+film_id)\n",
    "                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                    logger.debug('Page content not valid '+film_id)\n",
    "                item_texts=['None']\n",
    "                item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                item_df['film_id'] = film_id\n",
    "                all_dfs_synopsis.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping Synopsis\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%plotsummary%'\", 10000, process_synopsis)\n",
    "\n",
    "all_plots_df_syp =  pd.concat(all_dfs_synopsis)\n",
    "all_plots_df_syp.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_synopsis.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a33ee-1438-4aa7-8c02-3bb4ea6a9b40",
   "metadata": {},
   "source": [
    "### Generate Location CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d77dea-f72c-4f22-8450-af99da3f4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_locations = []\n",
    "\n",
    "#scan pages\n",
    "def process_locations(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items=soup.find('div',class_='sc-978e9339-1 ihWZgK ipc-page-grid__item ipc-page-grid__item--span-2')\n",
    "        locations=[]\n",
    "        if items:\n",
    "            items1 = items.find_all('section')\n",
    "            if len(items1) == 0:\n",
    "                logger.debug('No locations found '+film_id)\n",
    "                assert \"have any filming & production for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "            for item in items1:\n",
    "                texts=item.find('span')\n",
    "                if texts:\n",
    "                    texts=item.find('span').get_text()\n",
    "                    if texts==\"Filming locations\":\n",
    "                        items = item.find_all(class_=\"sc-bec740f7-5 eLQUFg\")\n",
    "                        for p in items:\n",
    "                            locations.append(p.text.strip())                   \n",
    "\n",
    "        if (not items) or (len(locations) == 0):\n",
    "            logger.debug('No locations found '+film_id)\n",
    "            if \"have any filming & production for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "            locations.append('None')\n",
    "            \n",
    "        locations_df = pd.DataFrame(locations, columns=['locations'])\n",
    "        locations_df['film_id'] = film_id\n",
    "        \n",
    "        all_locations.append(locations_df)\n",
    "\n",
    "logger.info(\">>> Scraping locations\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%locations%'\", 10000, process_locations)\n",
    "all_locations_df = pd.concat(all_locations)\n",
    "all_locations_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_locations.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc958196-6850-49c0-aac0-9c550eb22a2e",
   "metadata": {},
   "source": [
    "### Generate User Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc916366-cf31-42ae-b18a-47c316f574e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_user_reviews(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_titles = []\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text and title elements of all reviews\n",
    "        box = soup.find('div', class_='lister-list')\n",
    "        # check if page information is valid or not\n",
    "        if box:\n",
    "            items = box.findAll(class_=\"lister-item\")\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    title = item.find(class_=\"title\").get_text()\n",
    "                    if title:\n",
    "                        all_titles.append(title)\n",
    "                    else:\n",
    "                        all_titles.append('None')\n",
    "                    review = item.find(class_=\"text\").get_text(separator=' ', strip=True)\n",
    "                    if review:\n",
    "                        all_reviews.append(review)\n",
    "                    else:\n",
    "                        all_reviews.append('None')\n",
    "            else:\n",
    "                logger.debug('No user reviews found '+film_id)\n",
    "                all_titles.append('None')\n",
    "                all_reviews.append('None')    \n",
    "        else:\n",
    "            logger.debug('No user reviews found '+film_id)\n",
    "            if \"have any user reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_titles.append('None')\n",
    "                all_reviews.append('None')    \n",
    "               \n",
    "        my_dict = {'title':all_titles, 'user_review':all_reviews}\n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['review_id'] = range(len(item_df))\n",
    "        item_df['review_id'] = item_df['review_id']+1\n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping user reviews\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%reviews?sort=curated%'\", 10000, process_user_reviews)\n",
    "all_plots_df = pd.concat(all_dfs)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_user_reviews.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbebfed-0721-4af5-9e80-a5216c86579d",
   "metadata": {},
   "source": [
    "### Generate Critic Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50f9b11-fc3c-46ec-8591-fc503c23db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "-ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_critics_reviws(df):\n",
    "    #reviews_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        all_scores=[]\n",
    "        all_titles = []\n",
    "        all_critics=[]\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text and title elements of all reviews\n",
    "        box = soup.find('div', class_='sc-f65f65be-0 bBlII')\n",
    "\n",
    "        # check if page information is valid or not\n",
    "        if not box:\n",
    "            logger.debug('No critic reviews found '+film_id)\n",
    "            if \"have any metacritic reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_scores.append('None')\n",
    "                all_titles.append('None')\n",
    "                all_critics.append('None')\n",
    "                all_reviews.append('None') \n",
    "        else:\n",
    "            items = box.find_all('li',class_=\"ipc-metadata-list__item\")\n",
    "            for item in items:\n",
    "                score=item.find('div',class_='sc-d8486f96-2').get_text()\n",
    "                all_scores.append(score)\n",
    "                titles = item.find('div', class_='sc-d8486f96-3 bvYFxQ')\n",
    "                title = titles.find('span').get_text()\n",
    "                if not title:\n",
    "                    title = titles.find('a').get_text()\n",
    "                    if not title:\n",
    "                        title = 'None'\n",
    "                all_titles.append(title)\n",
    "                critics=item.find_all('span', class_='sc-d8486f96-6 ifLlre')\n",
    "                if critics:\n",
    "                    critic=item.find('span', class_='sc-d8486f96-6 ifLlre').get_text()\n",
    "                else: \n",
    "                    critics=item.find_all('a')[1]\n",
    "                    if critics:\n",
    "                        for critic in critics:\n",
    "                            critic=item.find('a').get_text()\n",
    "                    else:\n",
    "                        critic='None'\n",
    "                all_critics.append(critic)\n",
    "                reviews = item.find_all('div')\n",
    "                if reviews:\n",
    "                    review = reviews[-1].get_text()\n",
    "                else:\n",
    "                    review='None'\n",
    "                all_reviews.append(review)\n",
    "    \n",
    "        my_dict = {'score':all_scores,'title':all_titles, 'critic_name':all_critics,'critic_review':all_reviews}\n",
    "        \n",
    "        if((len(my_dict['score'])==0) & (len(my_dict['title'])==0) \n",
    "           & (len(my_dict['critic_name'])==0) & (len(my_dict['critic_review'])==0)):\n",
    "            page_not_valid = True\n",
    "        else:\n",
    "            page_not_valid = False\n",
    "        \n",
    "        if page_not_valid:\n",
    "            logger.debug('No critic reviews found '+film_id)\n",
    "            if \"have any metacritic reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_scores.append('None')\n",
    "                all_titles.append('None')\n",
    "                all_critics.append('None')\n",
    "                all_reviews.append('None')\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['c_review_id'] = range(len(item_df))\n",
    "        item_df['c_review_id'] = item_df['c_review_id']+1\n",
    "        all_dfs.append(item_df)\n",
    "  \n",
    "logger.info(\">>> Scraping critic reviews\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%criticreviews%'\", 10000, process_critics_reviws)\n",
    "all_criticreviews_df = pd.concat(all_dfs)\n",
    "all_criticreviews_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_critic_reviews.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc3970-dcc2-435c-bd61-fa1f22e5fa40",
   "metadata": {},
   "source": [
    "### Generate base details TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f629feb-b12c-4ad4-8d67-35c6e19dbba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#work on this\n",
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_details(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_detail_item=[]\n",
    "        all_detail_result = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract items of all details\n",
    "        items=soup.find('div',class_='sc-978e9339-1 ihWZgK ipc-page-grid__item ipc-page-grid__item--span-2')\n",
    "        if items:\n",
    "            items1 = items.find_all('section')\n",
    "            if len(items1) == 0:\n",
    "                assert \"have any detail for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "     \n",
    "            for item in items1:\n",
    "                texts=item.find('span')\n",
    "                if texts:\n",
    "                    texts=item.find('span').get_text()\n",
    "                    if texts==\"Details\":\n",
    "                        rows = item.find_all('li',class_='ipc-metadata-list__item')[:-1]\n",
    "                        for row in rows:\n",
    "                            detail_item=row.find('a',class_='ipc-metadata-list-item__label')\n",
    "                            if detail_item:\n",
    "                                detail_item=row.find('a',class_='ipc-metadata-list-item__label').get_text()\n",
    "                            else:\n",
    "                                detail_item=row.find('span',class_='ipc-metadata-list-item__label').get_text()\n",
    "    \n",
    "                            result_lists = row.find_all('li', class_='ipc-inline-list__item')\n",
    "                            for result_list in result_lists:\n",
    "                                detail_result = result_list.find('a', class_='ipc-metadata-list-item__list-content-item')\n",
    "                                if detail_result:\n",
    "                                    detail_result = result_list.find('a', class_='ipc-metadata-list-item__list-content-item').get_text()\n",
    "                                else:\n",
    "                                    detail_result = result_list.find('span', class_='ipc-metadata-list-item__list-content-item').get_text()\n",
    "                                all_detail_result.append(detail_result)\n",
    "                                all_detail_item.append(detail_item)\n",
    "                                \n",
    "        else:\n",
    "            logger.debug('No base details found '+film_id)\n",
    "            if \"have any details for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_detail_result.append('None')\n",
    "                all_detail_item.append('None')                             \n",
    "\n",
    "        if len(all_detail_item) == 0:\n",
    "            all_detail_item.append('None')\n",
    "        if len(all_detail_result) == 0:\n",
    "            all_detail_result.append('None')\n",
    "    \n",
    "        my_dict = {'detail_item':all_detail_item,'detail_result':all_detail_result}       \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id      \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping base details\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%title%ttfc_fc_tt%'\", 10000, process_details)\n",
    "all_details_df = pd.concat(all_dfs)\n",
    "all_details_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/films_base_details.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe9183-e65d-473d-88b1-bfc6e82f93a5",
   "metadata": {},
   "source": [
    "### Generate Awards TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1636d-0dd9-43e1-8c1e-2b0a9b076556",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_awards(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_events = []\n",
    "        all_winner_nominee=[]\n",
    "        all_award_name=[]\n",
    "        all_item=[]\n",
    "        all_name=[]\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text and title elements of all reviews\n",
    "        items = soup.find_all('section', class_='ipc-page-section')[:-3]\n",
    "         \n",
    "        if items:\n",
    "            for item1 in items:\n",
    "                h3_tags=item1.find('h3', class_='ipc-title__text')\n",
    "                if h3_tags: \n",
    "                    h3_tags=item1.find('h3', class_='ipc-title__text').get_text()\n",
    "                    subsection=item1.find_all('li',class_='ipc-metadata-list-summary-item')\n",
    "                    if subsection:\n",
    "                        for li in subsection:\n",
    "                            winner_nominee = li.find('a',class_=\"ipc-metadata-list-summary-item__t\")\n",
    "                            if winner_nominee:                                    \n",
    "                                winner_nominee = li.find('a',class_=\"ipc-metadata-list-summary-item__t\").get_text()\n",
    "                                award_name=li.find('span',class_='ipc-metadata-list-summary-item__tst')\n",
    "                                if award_name:                                        \n",
    "                                    award_name=li.find('span',class_='ipc-metadata-list-summary-item__tst').get_text()\n",
    "                                    types=li.find('span',class_='ipc-metadata-list-summary-item__li')\n",
    "                                    if types:\n",
    "                                        types=li.find('span',class_='ipc-metadata-list-summary-item__li').get_text()\n",
    "                                        allnames=li.find_all('li',class_='ipc-inline-list__item')[1:]\n",
    "                                        if allnames:\n",
    "                                            for allname in allnames:\n",
    "                                                name=allname.find('a',class_='ipc-metadata-list-summary-item__li').get_text()\n",
    "                                                all_name.append(name)\n",
    "                                                all_item.append(types)\n",
    "                                                all_award_name.append(award_name)\n",
    "                                                all_winner_nominee.append(winner_nominee)\n",
    "                                                all_events.append(h3_tags)\n",
    "                                        else:\n",
    "                                            name=None\n",
    "                                            all_name.append(name)\n",
    "                                            all_item.append(types)\n",
    "                                            all_award_name.append(award_name)\n",
    "                                            all_winner_nominee.append(winner_nominee)\n",
    "                                            all_events.append(h3_tags)\n",
    "                                    else:\n",
    "                                        types=None\n",
    "                                        name=None\n",
    "                                        all_name.append(name)\n",
    "                                        all_item.append(types)\n",
    "                                        all_award_name.append(award_name)\n",
    "                                        all_winner_nominee.append(winner_nominee)\n",
    "                                        all_events.append(h3_tags)\n",
    "\n",
    "\n",
    "\n",
    "        my_dict = {'event':all_events, 'winner_nominee':all_winner_nominee,'award_name':all_award_name,'item':all_item,'name':all_name}\n",
    "        \n",
    "        if((len(my_dict['event'])==0) & (len(my_dict['winner_nominee'])==0) & (len(my_dict['award_name'])==0)\n",
    "           & (len(my_dict['item'])==0) & (len(my_dict['name'])==0)):\n",
    "            page_not_valid = True\n",
    "        else:\n",
    "            page_not_valid = False\n",
    "        \n",
    "        if page_not_valid:\n",
    "            logger.debug('No awards found '+film_id)\n",
    "            if \"have any awards for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "            my_dict = {'event':['None'], 'winner_nominee':['None'],'award_name':['None'],'item':['None'],'name':['None']}\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id      \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping Awards\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%awards%'\", 10000, process_awards)\n",
    "all_awards_df = pd.concat(all_dfs)\n",
    "all_awards_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_awards.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79694d8c-5480-48a9-8797-c37d34d15c46",
   "metadata": {},
   "source": [
    "### Generate full_credits TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d6672a2-23fa-46a4-a323-6ce006cd877a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m         offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Connect to the database\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m db_conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/komal/Desktop/new_logs/films.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Define the SQL query\u001b[39;00m\n\u001b[0;32m     86\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM pages_dump WHERE url LIKE \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mullcredits\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# Function to extract writers and directors\n",
    "def process_full_credits(df):\n",
    "       \n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Define the categories to extract\n",
    "        categories = [\n",
    "            {\"name\": \"writer\", \"label\": \"Writer\"},\n",
    "            {\"name\": \"producer\", \"label\": \"Producer\"},\n",
    "            {\"name\": \"director\", \"label\": \"Director\"},\n",
    "            {\"name\": \"cinematographer\", \"label\": \"Cinematographer\"},\n",
    "            {\"name\": \"location_management\", \"label\": \"Location Manager\"},\n",
    "#            {\"name\": \"production_designer\", \"label\": \"Production Designer\"},\n",
    "#            {\"name\": \"art_director\", \"label\": \"Art Director\"},\n",
    "#            {\"name\": \"set_decorator\", \"label\": \"Set Decorator\"}\n",
    "        ]\n",
    "\n",
    "        # Loop through categories to extract data\n",
    "        for category in categories:\n",
    "            # Extract category\n",
    "            section = soup.find(\"h4\", attrs={\"name\": category[\"name\"]})\n",
    "            if section:\n",
    "                table = section.find_next(\"table\", class_=\"simpleCreditsTable\")\n",
    "                if table:\n",
    "                    credits = [clean_text(writer.text) for writer in table.find_all(\"td\", class_=\"name\")]\n",
    "                    credits_df = pd.DataFrame(credits, columns=['person'])\n",
    "                    credits_df['role'] = category[\"label\"]\n",
    "                    credits_df['film_id'] = film_id\n",
    "                    all_dfs.append(credits_df)                \n",
    "                                   \n",
    "                else:\n",
    "                    logger.debug(f\"No {category['name']} table found \"+film_id)\n",
    "                    item_texts=['None']\n",
    "                    item_df = pd.DataFrame(item_texts, columns=['person'])\n",
    "                    item_df['film_id'] = film_id\n",
    "                    item_df['role'] = category[\"label\"]\n",
    "                    all_dfs.append(item_df)                    \n",
    "            else:\n",
    "                logger.debug(f\"No {category['name']} details found \"+film_id)\n",
    "                item_texts=['None']\n",
    "                item_df = pd.DataFrame(item_texts, columns=['person'])\n",
    "                item_df['film_id'] = film_id\n",
    "                item_df['role'] = category[\"label\"]\n",
    "                all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping full credits\")\n",
    "# Define the SQL query\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%fullcredits%'\", 10000, process_full_credits)\n",
    "all_cedits_df = pd.concat(all_dfs)\n",
    "all_cedits_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_credits.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0488a-e7c9-4e4d-81a7-1b8cd5430ad0",
   "metadata": {},
   "source": [
    "## Generate Box Office information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f56a345-d4c9-4b44-92e0-9b247c454718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 10000\n",
      "  offset = 20000\n",
      "  offset = 30000\n",
      "  offset = 40000\n",
      "  offset = 50000\n",
      "  offset = 60000\n",
      "  offset = 70000\n",
      "  offset = 80000\n",
      "  offset = 90000\n",
      "  offset = 100000\n",
      "  offset = 110000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#work on this\n",
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_details(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_detail_item=[]\n",
    "        all_detail_result = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract items of all details\n",
    "        items=soup.find('div',class_='sc-978e9339-1 ihWZgK ipc-page-grid__item ipc-page-grid__item--span-2')\n",
    "        if items:\n",
    "            items1 = items.find_all('section')\n",
    "            if len(items1) == 0:\n",
    "                assert \"have any detail for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "            for item in items1:\n",
    "                texts=item.find('span')\n",
    "                if texts:\n",
    "                    texts=item.find('span').get_text()\n",
    "                    if texts==\"Box office\":\n",
    "                        rows = item.find_all('li',class_='ipc-metadata-list__item')\n",
    "                        for row in rows:\n",
    "                            detail_item=row.find('span',class_='ipc-metadata-list-item__label').get_text()\n",
    "                            result_lists = row.find_all('li', class_='ipc-inline-list__item')\n",
    "                            for result_list in result_lists:\n",
    "                                detail_result = result_list.find('span', class_='ipc-metadata-list-item__list-content-item').get_text()\n",
    "                                all_detail_result.append(detail_result)\n",
    "                                all_detail_item.append(detail_item)\n",
    "                                \n",
    "        else:\n",
    "            logger.debug('No box office details found '+film_id)\n",
    "            if \"have any details for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_detail_result.append('None')\n",
    "                all_detail_item.append('None')                             \n",
    "\n",
    "        if len(all_detail_item) == 0:\n",
    "            all_detail_item.append('None')\n",
    "        if len(all_detail_result) == 0:\n",
    "            all_detail_result.append('None')\n",
    "    \n",
    "        my_dict = {'detail_item':all_detail_item,'detail_result':all_detail_result}       \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id      \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping base details\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%title%ttfc_fc_tt%'\", 10000, process_details)\n",
    "#temp_offset(db_conn, \"select * from pages_dump where url like '%title%ttfc_fc_tt%'\", 5, process_details)\n",
    "\n",
    "all_details_df = pd.concat(all_dfs)\n",
    "all_details_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/films_box_office.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfa65f-2bb0-4014-a463-162e1390ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-DL-TF2",
   "language": "python",
   "name": "py3-dl-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
