{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0e1f3b-3ffc-492e-8b98-8f09c678825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cell\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import gzip\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57ed2-187a-41a7-9f52-8b9c4a60a123",
   "metadata": {},
   "source": [
    "## Extract data from HTML from sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408eba0c-3580-4d74-b864-c3060d367a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from db import scan_table_limit_offset, decompress_string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "db_conn = sqlite3.connect(os.path.join(os.path.dirname(__name__), \"sample_data/films.db\"))\n",
    "\n",
    "def get_film_id_from_url(url):\n",
    "    film_id = url.split('/')[4]\n",
    "    assert film_id[0:2] == 'tt'\n",
    "    return film_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc62b70-a4a3-4fd5-b25e-83295f2b3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logger():\n",
    "    # Create logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    log_folder = os.path.join(os.path.dirname(__name__), \"logs\")\n",
    "    if not os.path.isdir(log_folder):\n",
    "        os.mkdir(log_folder)\n",
    "        \n",
    "    log_file = os.path.join(log_folder, f\"s_log_{current_time}.txt\")\n",
    "\n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatter and add it to the handler\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the file handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61111423-a709-4cf3-a105-01cc173ea32a",
   "metadata": {},
   "source": [
    "Generate keyword CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f56da4-d403-4e2f-9e01-94b8cbef9e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_keywords(df):\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"ipc-metadata-list-summary-item__tc\")\n",
    "        keywords = [p.text.strip() for p in items]\n",
    "        if len(keywords) == 0:\n",
    "            logger.debug('No keywords found '+film_id)\n",
    "            if \"have any plot keywords for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                keywords.append('None')\n",
    "        keywords_df = pd.DataFrame(keywords, columns=['keyword'])\n",
    "        keywords_df['film_id'] = film_id\n",
    "        \n",
    "        all_dfs.append(keywords_df)\n",
    "\n",
    "\n",
    "logger.info(\">>> Scraping Keywords \")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%keywords%'\", 1000, process_keywords)\n",
    "all_keywords_df = pd.concat(all_dfs)\n",
    "all_keywords_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/film_keywords.csv\"), index=False)\n",
    "all_keywords_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/film_keywords.json\"), orient=\"records\")\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473e17d-3ef6-4a27-bc1a-d59ab1d7ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate plot summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6022ff-7763-43d1-8847-9b40f3913ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs_summary = []\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "# scan pages\n",
    "def process_plots(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"_counter\n",
    "        sectionA = soup.find_all('section', class_='ipc-page-section ipc-page-section--base')\n",
    "        for sectB in sectionA:\n",
    "            containerA = sectB.find('div', class_=\"ipc-title ipc-title--base ipc-title--section-title ipc-title--on-textPrimary\")\n",
    "\n",
    "            # check if page is valid\n",
    "            if containerA:\n",
    "                titleA = containerA.find('span').get_text()\n",
    "\n",
    "                # Extract Summaries\n",
    "                if titleA == 'Summaries':\n",
    "                    boxes = sectB.find_all(class_=\"ipc-html-content ipc-html-content--base\")\n",
    "\n",
    "                    # Check if summaries section has valid data\n",
    "                    if boxes:\n",
    "                        item_len = 0\n",
    "                        for box in boxes:\n",
    "                            item_len += 1\n",
    "                            items = box.find_all(class_=\"ipc-html-content-inner-div\")\n",
    "                            item_texts = [p.text.strip() for p in items]\n",
    "                            item_texts = [clean_summary(p) for p in item_texts]\n",
    "                            if len(item_texts) == 0:\n",
    "                                logger.debug('No summaries found '+film_id)\n",
    "                                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                                    logger.debug('Page content not valid '+film_id)\n",
    "                                    item_texts.append('None')\n",
    "                            item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                            item_df['film_id'] = film_id\n",
    "                            item_df['summary_id'] = item_len\n",
    "                            all_dfs_summary.append(item_df)\n",
    "                            \n",
    "                    else:    # If summaries section is not valid enter empty row for the film\n",
    "                        logger.debug('No summaries found '+film_id)\n",
    "                        if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                            logger.debug('Page content not valid '+film_id)\n",
    "                        item_texts=['None']\n",
    "                        item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                        item_df['film_id'] = film_id\n",
    "                        item_df['summary_id'] = 1\n",
    "                        all_dfs_summary.append(item_df)                       \n",
    "            \n",
    "            else:\n",
    "                # if page is not valid, enter empty row for summary and synopsis for the film\n",
    "                logger.debug('No plots found '+film_id)\n",
    "                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                    logger.debug('Page content not valid '+film_id)\n",
    "                item_texts=['None']\n",
    "                item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "                item_df['film_id'] = film_id\n",
    "                item_df['summary_id'] = 1\n",
    "                all_dfs_summary.append(item_df)\n",
    "\n",
    "\n",
    "logger.info(\">>> Scraping Plot Summary\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%plotsummary%'\", 1000, process_plots)\n",
    "all_plots_df = pd.concat(all_dfs_summary)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/film_plots.tsv\"), sep='\\t', index=False)\n",
    "all_plots_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/film_plots.json\"), orient=\"records\")\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688f419-cfd5-4b76-9b3f-a6b585eefad7",
   "metadata": {},
   "source": [
    "Generate synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079dd357-c66b-4628-be4a-8256413f2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs_synopsis = []\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "# scan pages\n",
    "def process_synopsis(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"_counter\n",
    "        sectionA = soup.find_all('section', class_='ipc-page-section ipc-page-section--base')\n",
    "        for sectB in sectionA:\n",
    "            containerA = sectB.find('div', class_=\"ipc-title ipc-title--base ipc-title--section-title ipc-title--on-textPrimary\")\n",
    "\n",
    "            # check if page is valid\n",
    "            if containerA:\n",
    "                titA = containerA.find('span').get_text()\n",
    "\n",
    "                if titA == 'Synopsis':\n",
    "                    boxes = sectB.find_all(class_=\"ipc-html-content ipc-html-content--base\")\n",
    "                    # Check if synopsis section has valid data\n",
    "                    if boxes:\n",
    "                        for box in boxes:\n",
    "                            items = box.find_all(class_=\"ipc-html-content-inner-div\")\n",
    "                            item_texts = [p.text.strip() for p in items]\n",
    "                            item_texts = [clean_summary(p) for p in item_texts]\n",
    "                            if len(item_texts) == 0:\n",
    "                                logger.debug('No synopsis found '+film_id)\n",
    "                                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                                    #logger.debug('Page content not valid '+film_id)\n",
    "                                    item_texts.append('None')\n",
    "                            item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                            item_df['film_id'] = film_id\n",
    "                            all_dfs_synopsis.append(item_df)\n",
    "                    else:\n",
    "                        # If synopsis section is not valid enter empty row for the film\n",
    "                        logger.debug('No synopsis found '+film_id)\n",
    "                        if \"have any synopsis for this title yet\" in html.unescape(html_content):\n",
    "                            logger.debug('Page content not valid '+film_id)\n",
    "                        item_texts=['None']\n",
    "                        item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                        item_df['film_id'] = film_id\n",
    "                        all_dfs_synopsis.append(item_df)\n",
    "                        \n",
    "            else:\n",
    "                # if page is not valid, enter empty row for summary and synopsis for the film\n",
    "                logger.debug('No plots found '+film_id)\n",
    "                if \"have any plot for this title yet\" in html.unescape(html_content):\n",
    "                    logger.debug('Page content not valid '+film_id)\n",
    "                item_texts=['None']\n",
    "                item_df = pd.DataFrame(item_texts, columns=['synopsis'])\n",
    "                item_df['film_id'] = film_id\n",
    "                all_dfs_synopsis.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping Synopsis\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%plotsummary%'\", 1000, process_synopsis)\n",
    "\n",
    "all_plots_df_syp =  pd.concat(all_dfs_synopsis)\n",
    "all_plots_df_syp.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/film_synopsis.tsv\"), sep='\\t', index=False)\n",
    "all_plots_df_syp.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/film_synopsis.json\"), orient=\"records\")\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a33ee-1438-4aa7-8c02-3bb4ea6a9b40",
   "metadata": {},
   "source": [
    "Generate Location CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d77dea-f72c-4f22-8450-af99da3f4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_locations = []\n",
    "\n",
    "#scan pages\n",
    "def process_locations(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"sc-bec740f7-5 eLQUFg\")\n",
    "        locations = [p.text.strip() for p in items]\n",
    "        \n",
    "        if len(locations) == 0:\n",
    "            logger.debug('No locations found '+film_id)\n",
    "            if \"have any filming & production for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                locations.append('None')\n",
    "        locations_df = pd.DataFrame(locations, columns=['locations'])\n",
    "        locations_df['film_id'] = film_id\n",
    "        \n",
    "        all_locations.append(locations_df)\n",
    "\n",
    "logger.info(\">>> Scraping locations\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%locations%'\", 1000, process_locations)\n",
    "all_locations_df = pd.concat(all_locations)\n",
    "all_locations_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/filming_locations.csv\"), index=False)\n",
    "all_locations_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/filming_locations.json\"), orient=\"records\")\n",
    "print(\"ok\")\n",
    "\n",
    "del all_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc958196-6850-49c0-aac0-9c550eb22a2e",
   "metadata": {},
   "source": [
    "Generate User Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc916366-cf31-42ae-b18a-47c316f574e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "# scan pages\n",
    "def process_user_reviews(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_titles = []\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text and title elements of all reviews\n",
    "        box = soup.find('div', class_='lister-list')\n",
    "        # check if page information is valid or not\n",
    "        if box:\n",
    "            items = box.findAll(class_=\"lister-item\")\n",
    "            if items:\n",
    "                for item in items:\n",
    "                    title = item.find(class_=\"title\").get_text()\n",
    "                    if title:\n",
    "                        all_titles.append(title)\n",
    "                    else:\n",
    "                        all_titles.append('None')\n",
    "                    review = item.find(class_=\"text\").get_text(separator=' ', strip=True)\n",
    "                    if review:\n",
    "                        all_reviews.append(review)\n",
    "                    else:\n",
    "                        all_reviews.append('None')\n",
    "            else:\n",
    "                logger.debug('No user reviews found '+film_id)\n",
    "                all_titles.append('None')\n",
    "                all_reviews.append('None')    \n",
    "        else:\n",
    "            logger.debug('No user reviews found '+film_id)\n",
    "            if \"have any user reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_titles.append('None')\n",
    "                all_reviews.append('None')    \n",
    "               \n",
    "        my_dict = {'title':all_titles, 'user_review':all_reviews}\n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['review_id'] = range(len(item_df))\n",
    "        item_df['review_id'] = item_df['review_id']+1\n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping user reviews\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%reviews?sort=curated%'\", 1000, process_user_reviews)\n",
    "all_plots_df = pd.concat(all_dfs)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/user_reviews.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "all_plots_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/user_reviews.json\"), orient=\"records\")\n",
    "print(\"ok\")\n",
    "\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbebfed-0721-4af5-9e80-a5216c86579d",
   "metadata": {},
   "source": [
    "Generate Critic Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50f9b11-fc3c-46ec-8591-fc503c23db79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "-ok\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_critics_reviws(df):\n",
    "    #reviews_counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        all_scores=[]\n",
    "        all_titles = []\n",
    "        all_critics=[]\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text and title elements of all reviews\n",
    "        box = soup.find('div', class_='sc-f65f65be-0 bBlII')\n",
    "\n",
    "        # check if page information is valid or not\n",
    "        if not box:\n",
    "            logger.debug('No critic reviews found '+film_id)\n",
    "            if \"have any metacritic reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_scores.append('None')\n",
    "                all_titles.append('None')\n",
    "                all_critics.append('None')\n",
    "                all_reviews.append('None') \n",
    "        else:\n",
    "            items = box.find_all('li',class_=\"ipc-metadata-list__item\")\n",
    "            for item in items:\n",
    "                score=item.find('div',class_='sc-d8486f96-2').get_text()\n",
    "                all_scores.append(score)\n",
    "                titles = item.find('div', class_='sc-d8486f96-3 bvYFxQ')\n",
    "                title = titles.find('span').get_text()\n",
    "                if not title:\n",
    "                    title = titles.find('a').get_text()\n",
    "                    if not title:\n",
    "                        title = 'None'\n",
    "                all_titles.append(title)\n",
    "                critics=item.find_all('span', class_='sc-d8486f96-6 ifLlre')\n",
    "                if critics:\n",
    "                    critic=item.find('span', class_='sc-d8486f96-6 ifLlre').get_text()\n",
    "                else: \n",
    "                    critics=item.find_all('a')[1]\n",
    "                    if critics:\n",
    "                        for critic in critics:\n",
    "                            critic=item.find('a').get_text()\n",
    "                    else:\n",
    "                        critic=None\n",
    "                all_critics.append(critic)\n",
    "                reviews = item.find_all('div')\n",
    "                if reviews:\n",
    "                    review = reviews[-1].get_text()\n",
    "                else:\n",
    "                    review=None\n",
    "                all_reviews.append(review)\n",
    "    \n",
    "        my_dict = {'score':all_scores,'title':all_titles, 'critic_name':all_critics,'critic_review':all_reviews}    \n",
    "        if len(my_dict) == 0:\n",
    "            logger.debug('No critic reviews found '+film_id)\n",
    "            if \"have any metacritic reviews for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_scores.append('None')\n",
    "                all_titles.append('None')\n",
    "                all_critics.append('None')\n",
    "                all_reviews.append('None')\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['c_review_id'] = range(len(item_df))\n",
    "        item_df['c_review_id'] = item_df['c_review_id']+1\n",
    "        all_dfs.append(item_df)\n",
    "  \n",
    "logger.info(\">>> Scraping critic reviews\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%criticreviews%'\", 1000, process_critics_reviws)\n",
    "all_criticreviews_df = pd.concat(all_dfs)\n",
    "all_criticreviews_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data\\critic_reviews.tsv\"), sep='\\t', index=False)\n",
    "all_criticreviews_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/critic_reviews.json\"), orient=\"records\")\n",
    "print(\"-ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc3970-dcc2-435c-bd61-fa1f22e5fa40",
   "metadata": {},
   "source": [
    "Generate base details CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f629feb-b12c-4ad4-8d67-35c6e19dbba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan_table_limit_offset\n",
      "  offset = 0\n",
      "  offset = 1000\n",
      "  offset = 2000\n",
      "  offset = 3000\n",
      "  offset = 4000\n",
      "  offset = 5000\n",
      "  offset = 6000\n",
      "  offset = 7000\n",
      "  offset = 8000\n",
      "  offset = 9000\n",
      "  offset = 10000\n",
      "  offset = 11000\n",
      "  offset = 12000\n",
      "  offset = 13000\n",
      "  offset = 14000\n",
      "  offset = 15000\n",
      "  offset = 16000\n",
      "  offset = 17000\n",
      "  offset = 18000\n",
      "  offset = 19000\n",
      "  offset = 20000\n",
      "  offset = 21000\n",
      "  offset = 22000\n",
      "  offset = 23000\n",
      "  offset = 24000\n",
      "  offset = 25000\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#work on this\n",
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_details(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_detail_item=[]\n",
    "        all_detail_result = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract items of all details\n",
    "        items=soup.find('div',class_='sc-978e9339-1 ihWZgK ipc-page-grid__item ipc-page-grid__item--span-2')\n",
    "        if items:\n",
    "            items1 = items.find_all('section')\n",
    "            if len(items1) == 0:\n",
    "                assert \"have any detail for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "     \n",
    "            for item in items1:\n",
    "                texts=item.find('span')\n",
    "                if texts:\n",
    "                    texts=item.find('span').get_text()\n",
    "                    if texts==\"Details\":\n",
    "                        rows = item.find_all('li',class_='ipc-metadata-list__item')[:-1]\n",
    "                        for row in rows:\n",
    "                            detail_item=row.find('a',class_='ipc-metadata-list-item__label')\n",
    "                            if detail_item:\n",
    "                                detail_item=row.find('a',class_='ipc-metadata-list-item__label').get_text()\n",
    "                            else:\n",
    "                                detail_item=row.find('span',class_='ipc-metadata-list-item__label').get_text()\n",
    "    \n",
    "                            result_lists = row.find_all('li', class_='ipc-inline-list__item')\n",
    "                            for result_list in result_lists:\n",
    "                                detail_result = result_list.find('a', class_='ipc-metadata-list-item__list-content-item')\n",
    "                                if detail_result:\n",
    "                                    detail_result = result_list.find('a', class_='ipc-metadata-list-item__list-content-item').get_text()\n",
    "                                else:\n",
    "                                    detail_result = result_list.find('span', class_='ipc-metadata-list-item__list-content-item').get_text()\n",
    "                                all_detail_result.append(detail_result)\n",
    "                                all_detail_item.append(detail_item)\n",
    "                                \n",
    "        else:\n",
    "            logger.debug('No base details found '+film_id)\n",
    "            if \"have any details for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                all_detail_result.append('None')\n",
    "                all_detail_item.append('None')                             \n",
    "    \n",
    "        my_dict = {'detail_item':all_detail_item,'detail_result':all_detail_result}       \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id      \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping base details\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%title%ttfc_fc_tt%'\", 1000, process_details)\n",
    "all_details_df = pd.concat(all_dfs)\n",
    "all_details_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data/films_base_details.csv\"), index=False)\n",
    "all_details_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data/films_base_details.json\"), orient=\"records\")\n",
    "\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a34afc-ab28-42a1-9f03-71dccfae822f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ebe9183-e65d-473d-88b1-bfc6e82f93a5",
   "metadata": {},
   "source": [
    "Generate Awards TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1636d-0dd9-43e1-8c1e-2b0a9b076556",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_awards(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_events = []\n",
    "        all_winner_nominee=[]\n",
    "        all_award_name=[]\n",
    "        all_item=[]\n",
    "        all_name=[]\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text and title elements of all reviews\n",
    "        items = soup.find_all('section', class_='ipc-page-section')[:-3]\n",
    "        \n",
    "        if len(items) == 0:\n",
    "\n",
    "                        \n",
    "        if items:\n",
    "            for item1 in items:\n",
    "                h3_tags=item1.find('h3', class_='ipc-title__text')\n",
    "                if h3_tags: \n",
    "                    h3_tags=item1.find('h3', class_='ipc-title__text').get_text()\n",
    "                    subsection=item1.find_all('li',class_='ipc-metadata-list-summary-item')\n",
    "                    if subsection:\n",
    "                        for li in subsection:\n",
    "                            winner_nominee = li.find('a',class_=\"ipc-metadata-list-summary-item__t\")\n",
    "                            if winner_nominee:                                    \n",
    "                                winner_nominee = li.find('a',class_=\"ipc-metadata-list-summary-item__t\").get_text()\n",
    "                                award_name=li.find('span',class_='ipc-metadata-list-summary-item__tst')\n",
    "                                if award_name:                                        \n",
    "                                    award_name=li.find('span',class_='ipc-metadata-list-summary-item__tst').get_text()\n",
    "                                    types=li.find('span',class_='ipc-metadata-list-summary-item__li')\n",
    "                                    if types:\n",
    "                                        types=li.find('span',class_='ipc-metadata-list-summary-item__li').get_text()\n",
    "                                        allnames=li.find_all('li',class_='ipc-inline-list__item')[1:]\n",
    "                                        if allnames:\n",
    "                                            for allname in allnames:\n",
    "                                                name=allname.find('a',class_='ipc-metadata-list-summary-item__li').get_text()\n",
    "                                                all_name.append(name)\n",
    "                                                all_item.append(types)\n",
    "                                                all_award_name.append(award_name)\n",
    "                                                all_winner_nominee.append(winner_nominee)\n",
    "                                                all_events.append(h3_tags)\n",
    "                                        else:\n",
    "                                            name=None\n",
    "                                            all_name.append(name)\n",
    "                                            all_item.append(types)\n",
    "                                            all_award_name.append(award_name)\n",
    "                                            all_winner_nominee.append(winner_nominee)\n",
    "                                            all_events.append(h3_tags)\n",
    "                                    else:\n",
    "                                        types=None\n",
    "                                        name=None\n",
    "                                        all_name.append(name)\n",
    "                                        all_item.append(types)\n",
    "                                        all_award_name.append(award_name)\n",
    "                                        all_winner_nominee.append(winner_nominee)\n",
    "                                        all_events.append(h3_tags)\n",
    "\n",
    "\n",
    "\n",
    "        my_dict = {'event':all_events, 'winner_nominee':all_winner_nominee,'award_name':all_award_name,'item':all_item,'name':all_name}\n",
    "        if len(my_dict) == 0:\n",
    "            logger.debug('No awards found '+film_id)\n",
    "            if \"have any awards for this title yet\" in html.unescape(html_content):\n",
    "                logger.debug('Page content not valid '+film_id)\n",
    "                my_dict = {'event':'None', 'winner_nominee':'None','award_name':'None','item':'None','name':'None'}\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id      \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "logger.info(\">>> Scraping Awards\")\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%awards%'\", 1000, process_awards)\n",
    "all_awards_df = pd.concat(all_dfs)\n",
    "all_awards_df.to_csv(os.path.join(os.path.dirname(__name__), \"sample_data\\awards.tsv\"), sep='\\t', index=False)\n",
    "all_awards_df.to_json(os.path.join(os.path.dirname(__name__), \"sample_data\\awards.json\"), sep='\\t', index=False)\n",
    "\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e6dca-31a5-4cd8-80a8-3330c5517e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79694d8c-5480-48a9-8797-c37d34d15c46",
   "metadata": {},
   "source": [
    "Generate full_credits CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d6672a2-23fa-46a4-a323-6ce006cd877a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m         offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m limit\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Connect to the database\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m db_conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/komal/Desktop/new_logs/films.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Define the SQL query\u001b[39;00m\n\u001b[0;32m     86\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM pages_dump WHERE url LIKE \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mullcredits\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract writers and directors\n",
    "def process_writers_and_directors(df):\n",
    "    all_writers_directors = []\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract writers\n",
    "        writers_section = soup.find(\"h4\", attrs={\"name\": \"writer\"})\n",
    "        if writers_section:\n",
    "            writers_table = writers_section.find_next(\"table\", class_=\"simpleCreditsTable\")\n",
    "            if writers_table:\n",
    "                writers = [clean_text(writer.text) for writer in writers_table.find_all(\"td\", class_=\"name\")]\n",
    "                \n",
    "                # Create DataFrame for writers\n",
    "                writers_df = pd.DataFrame({\"person\": writers, \"role\": \"Writer\"})\n",
    "                \n",
    "                # Add film ID to DataFrame\n",
    "                writers_df['film_id'] = film_id\n",
    "                \n",
    "                # Append DataFrame to list\n",
    "                all_writers_directors.append(writers_df)\n",
    "            else:\n",
    "                print(f\"Writers table not found for film ID: {film_id}\")\n",
    "        else:\n",
    "            print(f\"Writers section not found for film ID: {film_id}\")\n",
    "        \n",
    "        # Extract directors\n",
    "        directors_section = soup.find(\"h4\", attrs={\"name\": \"director\"})\n",
    "        if directors_section:\n",
    "            directors_table = directors_section.find_next(\"table\", class_=\"simpleCreditsTable\")\n",
    "            if directors_table:\n",
    "                directors = [clean_text(director.text) for director in directors_table.find_all(\"td\", class_=\"name\")]\n",
    "                \n",
    "                # Create DataFrame for directors\n",
    "                directors_df = pd.DataFrame({\"person\": directors, \"role\": \"Director\"})\n",
    "                \n",
    "                # Add film ID to DataFrame\n",
    "                directors_df['film_id'] = film_id\n",
    "                \n",
    "                # Append DataFrame to list\n",
    "                all_writers_directors.append(directors_df)\n",
    "            else:\n",
    "                print(f\"Directors table not found for film ID: {film_id}\")\n",
    "        else:\n",
    "            print(f\"Directors section not found for film ID: {film_id}\")\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    if all_writers_directors:\n",
    "        all_writers_directors_df = pd.concat(all_writers_directors)\n",
    "        all_writers_directors_df.to_csv('/Users/komal/Desktop/new_logs/directors_writers.csv', index=False)\n",
    "        print(\"Directors and Writers data saved to CSV file.\")\n",
    "    else:\n",
    "        print(\"No Directors and Writers data found.\")\n",
    "\n",
    "logger.info(\">>> Scraping full credits\")\n",
    "# Define the SQL query\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%fullcredits%'\", 1000, process_writers_and_directors)\n",
    "all_criticreviews_df = pd.concat(all_dfs)\n",
    "all_criticreviews_df.to_csv(os.path.join(os.path.dirname(__name__), \"directors_writers.csv\"), sep='\\t', index=False)\n",
    "#print(f\"Total Movies with no locations = {nolocation_counter}\")\n",
    "print(\"ok\")\n",
    "del all_dfs\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Read the existing CSV file into a DataFrame\n",
    "existing_df = pd.read_csv('/Users/komal/Desktop/new_logs/directors_writers.csv')\n",
    "\n",
    "# Drop any duplicate rows\n",
    "existing_df = existing_df.drop_duplicates()\n",
    "\n",
    "# Pivot the DataFrame to reshape it\n",
    "pivoted_df = existing_df.pivot_table(index='film_id', columns='role', values='person', aggfunc=lambda x: ', '.join(x)).reset_index()\n",
    "pivoted_df.columns.name = None  # Remove the column name for better formatting\n",
    "\n",
    "# Print the reshaped DataFrame\n",
    "pivoted_df.head(20)\n",
    "\n",
    "pivoted_df.to_csv('/Users/komal/Desktop/new_logs/directors_writers_reshaped.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfa65f-2bb0-4014-a463-162e1390ee5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-DL-TF2",
   "language": "python",
   "name": "py3-dl-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
