{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e1f3b-3ffc-492e-8b98-8f09c678825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cell\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import gzip\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd57ed2-187a-41a7-9f52-8b9c4a60a123",
   "metadata": {},
   "source": [
    "## Extract data from HTML from sqlite db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408eba0c-3580-4d74-b864-c3060d367a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from db import scan_table_limit_offset, decompress_string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "db_conn = sqlite3.connect(os.path.join(os.path.dirname(__name__), \"data/films.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61111423-a709-4cf3-a105-01cc173ea32a",
   "metadata": {},
   "source": [
    "Generate keyword CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f56da4-d403-4e2f-9e01-94b8cbef9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# scan pages\n",
    "def process_keywords(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"ipc-metadata-list-summary-item__tc\")\n",
    "        keywords = [p.text.strip() for p in items]\n",
    "        if len(keywords) == 0:\n",
    "            assert \"have any plot keywords for this title yet\" in html_content.lower()\n",
    "            continue\n",
    "        keywords_df = pd.DataFrame(keywords, columns=['keyword'])\n",
    "        keywords_df['film_id'] = film_id\n",
    "        \n",
    "        all_dfs.append(keywords_df)\n",
    "\n",
    "\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%keywords%'\", 1000, process_keywords)\n",
    "all_keywords_df = pd.concat(all_dfs)\n",
    "all_keywords_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_keywords.csv\"), index=False)\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1cfc94-c6bf-42e6-a936-235170a19184",
   "metadata": {},
   "source": [
    "Generate plot summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ecde1d-9868-4e25-a8d3-eade9fda050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "# scan pages\n",
    "def process_plots(df):\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"ipc-html-content-inner-div\")\n",
    "        item_texts = [p.text.strip() for p in items]\n",
    "        item_texts = [clean_summary(p) for p in item_texts]\n",
    "        if len(item_texts) == 0:\n",
    "            #assert \"have any plot summaries for this title yet\" in html_content.lower()\n",
    "            continue\n",
    "        item_df = pd.DataFrame(item_texts, columns=['summary'])\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['summary_id'] = range(len(item_df))\n",
    "        item_df['summary_id'] = item_df['summary_id']+1\n",
    "        \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%plotsummary%'\", 1000, process_plots)\n",
    "all_plots_df = pd.concat(all_dfs)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/film_plots.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a33ee-1438-4aa7-8c02-3bb4ea6a9b40",
   "metadata": {},
   "source": [
    "Generate Location CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d77dea-f72c-4f22-8450-af99da3f4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locations = []\n",
    "nolocation_counter = 0\n",
    "#scan pages\n",
    "def process_locations(df):\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract text of all elements with class=\"paragraph\"\n",
    "        items = soup.find_all(class_=\"sc-bec740f7-5 eLQUFg\")\n",
    "        locations = [p.text.strip() for p in items]\n",
    "        #print(f\"film_id = {film_id}, locations = {locations}\")\n",
    "        if len(locations) == 0:\n",
    "            nolocation_text = \"It looks like we don't have any filming & production for this title yet. Be the first to contribute.Learn more\"\n",
    "            containers = soup.findAll('article',class_=\"sc-b707829e-0 umohR\")\n",
    "            locations = [p.text.strip() for p in containers]\n",
    "            if (locations[0] != nolocation_text):\n",
    "                assert \"have any plot locations for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "            else:\n",
    "                counter+=1\n",
    "        locations_df = pd.DataFrame(locations, columns=['locations'])\n",
    "        locations_df['film_id'] = film_id\n",
    "        \n",
    "        all_locations.append(locations_df)\n",
    "    print(f\"Movies with no locations = {counter}\")\n",
    "    nolocation_counter += counter\n",
    "        \n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%locations%'\", 1000, process_locations)\n",
    "all_locations_df = pd.concat(all_locations)\n",
    "all_locations_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/filming_locations.csv\"), index=False)\n",
    "print(f\"Total films with no locations = {nolocation_counter}\")\n",
    "print(\"ok\")\n",
    "del all_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc958196-6850-49c0-aac0-9c550eb22a2e",
   "metadata": {},
   "source": [
    "Generate User Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc916366-cf31-42ae-b18a-47c316f574e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "def clean_summary(s):\n",
    "    s = s.replace('\\t', '    ').strip()\n",
    "    return s\n",
    "\n",
    "# scan pages\n",
    "def process_user_reviews(df):\n",
    "    for index, row in df.iterrows():\n",
    "        all_titles = []\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text and title elements of all reviews\n",
    "        box = soup.find('div', class_='lister-list')\n",
    "        items = box.findAll(class_=\"lister-item\")\n",
    "        for item in items:\n",
    "            title = item.find(class_=\"title\").get_text()\n",
    "            all_titles.append(title)\n",
    "            review = item.find(class_=\"text\").get_text(separator=' ', strip=True)\n",
    "            all_reviews.append(review)\n",
    "\n",
    "        my_dict = {'title':all_titles, 'user_review':all_reviews}\n",
    "\n",
    "        if len(my_dict) == 0:\n",
    "            assert \"have any user reviews for this title yet\" in html_content.lower()\n",
    "            continue\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['review_id'] = range(len(item_df))\n",
    "        item_df['review_id'] = item_df['review_id']+1\n",
    "        \n",
    "        all_dfs.append(item_df)\n",
    "\n",
    "\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%reviews?sort=curated%'\", 1000, process_user_reviews)\n",
    "all_plots_df = pd.concat(all_dfs)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/user_reviews.tsv\"), sep='\\t', index=False)\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4cfa0e-c435-48dd-921b-f8bdc61231f9",
   "metadata": {},
   "source": [
    "Generate Critic Reviews TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed09ee9-4f25-4095-a33a-6fe48ba54d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "def process_criticreviews(url, film_id):\n",
    "\n",
    "    path=os.path.join(os.path.dirname(__name__), \"chromedriver-win64\\chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(path)\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(5)  # ensure the page is fully loaded\n",
    "    html = driver.page_source\n",
    "    \n",
    "    page_soup = BeautifulSoup(html, \"html.parser\")\n",
    "    page_soup.prettify()\n",
    "    \n",
    "    reviewscore = []\n",
    "    logo = []\n",
    "    reviewquote = []\n",
    "    criticname = []\n",
    "\n",
    "    # Extract all critic reviews and apply loop to extract info from each review\n",
    "    uscore = page_soup.find(class_='c-siteReviewScore_background')\n",
    "    universal_metascore = uscore.find('span').get_text()\n",
    "\n",
    "    # Extract all critic reviews and apply loop to extract info from each review\n",
    "    boxA = page_soup.find('div', class_='c-pageProductReviews_row')\n",
    "    reviews = boxA.find_all(class_='c-siteReview')\n",
    "    \n",
    "    for review in reviews:\n",
    "        boxB = review.find('div', class_='c-siteReviewHeader_reviewScore')\n",
    "        reviewscore.append(boxB.find('span').get_text())\n",
    "        logo.append(review.find('a', class_='c-siteReviewHeader_publicationName').get_text(separator=' ', strip=True))\n",
    "        reviewquote.append(review.find('div', class_='c-siteReview_quote').get_text())\n",
    "        boxC = review.find('div', class_='c-siteReview_extra')\n",
    "        value = boxC.find('a', class_='c-siteReview_criticName')\n",
    "        if value is None:\n",
    "            criticname.append(\"\")\n",
    "        else:\n",
    "            criticname.append(value.get_text(separator=' ', strip=True))\n",
    "\n",
    "        my_dict = {'reviewscore':reviewscore, 'logo':logo, 'critic_review':reviewquote, 'critic_name':criticname}\n",
    "\n",
    "        if len(my_dict) == 0:\n",
    "            assert \"have any critic reviews for this title yet\" in html_content.lower()\n",
    "            continue\n",
    "        \n",
    "        item_df = pd.DataFrame.from_dict(my_dict)\n",
    "        item_df['film_id'] = film_id\n",
    "        item_df['universal_score'] = universal_metascore\n",
    "        item_df['review_id'] = range(len(item_df))\n",
    "        item_df['review_id'] = item_df['review_id']+1\n",
    "        \n",
    "        all_dfs.append(item_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cbbf5-e8cc-4a2a-8f8a-75f968a157b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noreview_counter = 0\n",
    "# scan pages\n",
    "def process_imdb_critics(df):\n",
    "    counter = 0\n",
    "    for index, row in df.iterrows():\n",
    "        all_titles = []\n",
    "        all_reviews = []\n",
    "        html_content = decompress_string(row['page_content_zip'])\n",
    "        film_id = get_film_id_from_url(row['url'])\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract href link of metacritics\n",
    "        containers = soup.findAll(class_=\"ipc-metadata-list-item__label ipc-metadata-list-item__label--link\")\n",
    "        href = [p['href'] for p in containers]\n",
    "        \n",
    "        if len(href) == 0:\n",
    "            noreview_text = \"It looks like we don't have any metacritic reviews for this title yet.\"\n",
    "            containers = soup.findAll('article',class_=\"sc-b707829e-0 umohR\")\n",
    "            no_review = [p.text.strip() for p in containers]\n",
    "            if (no_review[0] != noreview_text):\n",
    "                assert \"have any critic reviews for this title yet\" in html_content.lower()\n",
    "                continue\n",
    "            else:\n",
    "                counter+=1\n",
    "        website = re.sub(r'\\?.*',\"\",href[0]) + '/critic-reviews/'\n",
    "        process_criticreviews(website, film_id)\n",
    "    noreview_counter += counter\n",
    "\n",
    "scan_table_limit_offset(db_conn, \"select * from pages_dump where url like '%criticreviews/?ref_=tt_ov_rt%'\", 1000, process_imdb_critics)\n",
    "all_plots_df = pd.concat(all_dfs)\n",
    "all_plots_df.to_csv(os.path.join(os.path.dirname(__name__), \"data/critic_reviews.tsv\"), sep='\\t', index=False)\n",
    "print(f\"Total films with no reviews = {noreview_counter}\")\n",
    "print(\"ok\")\n",
    "del all_dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-DL-TF2",
   "language": "python",
   "name": "py3-dl-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
