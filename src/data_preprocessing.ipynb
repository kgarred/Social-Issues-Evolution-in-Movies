{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008dc291",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadaed8-52b9-488d-b5a6-dfad045480ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import scikitplot as skplt\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1ee872-0519-4872-b6cb-ea7954821a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\",category=(SettingWithCopyWarning))\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024bbe5",
   "metadata": {},
   "source": [
    "### Load necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.path.dirname(__name__), \"RawDataFiles/films_to_scrape_sample.csv\")\n",
    "script_df = pd.read_csv(filename, delimiter=',',low_memory=False)\n",
    "print(\"script_df: \",len(script_df.tconst.unique()))\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"RawDataFiles/film_plots.tsv\")\n",
    "plots_df = pd.read_csv(filename, delimiter='\\t', low_memory=False)\n",
    "print(\"plot_df: \",len(plots_df.film_id.unique()))\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"RawDataFiles/film_locations.tsv\")\n",
    "locations_df = pd.read_csv(filename, delimiter='\\t', low_memory=False)\n",
    "print(\"locations_df: \",len(locations_df.film_id.unique()))\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"RawDataFiles/films_base_details.tsv\")\n",
    "base_df = pd.read_csv(filename, delimiter='\\t', low_memory=False)\n",
    "print(\"base_df: \",len(base_df.film_id.unique()))\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"RawDataFiles/films_box_office.tsv\")\n",
    "box_df = pd.read_csv(filename, delimiter='\\t', low_memory=False)\n",
    "print(\"box_df: \",len(box_df.film_id.unique()))\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"imdb_scraper/Database/title.basics.tsv\")\n",
    "df4 = pd.read_csv(filename,delimiter='\\t', low_memory=False)\n",
    "print(\"title.basics: \",len(df4.tconst.unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f06918",
   "metadata": {},
   "source": [
    "### Drop Extra Films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_list = {'plots summary':plots_df,'locations':locations_df,'base details':base_df, 'box office':box_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9eef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_movies = ['tt15477076', 'tt0245936', 'tt15039908', 'tt0118656', 'tt0463670',\n",
    "                  'tt1606306', 'tt1298820', 'tt21441286', 'tt0487456', 'tt2536522', 'tt15698592', 'tt5910170','tt0081299']\n",
    "\n",
    "def remove_extra(df, unwanted_movies):\n",
    "    indexfilms = df[df['film_id'].isin(unwanted_movies)].index\n",
    "    df.drop(indexfilms, inplace=True)\n",
    "\n",
    "for key, value in db_list.items():\n",
    "    remove_extra(value, unwanted_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7e828",
   "metadata": {},
   "source": [
    "### Check and remove NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_of_films(df):\n",
    "    if 'tconst' in df:\n",
    "        x = len(df.tconst.unique())\n",
    "        #film_list = df.tconst.unique().tolist()\n",
    "    else:\n",
    "        x = len(df.film_id.unique())\n",
    "        #film_list = df.film_id.unique().tolist()\n",
    "    return x\n",
    "\n",
    "def calculate_null_data(df, item):\n",
    "    df[item] = np.where(df[item].isnull(), 'None', df[item])\n",
    "    Null_data = df[df[item]=='None']\n",
    "    return print_num_of_films(Null_data)\n",
    "\n",
    "null_dist = {'plots summary':0,'locations':0,'base details':0, 'box office':0}\n",
    "feature_item = ['summary','locations','detail_result','detail_result']\n",
    "\n",
    "print(\"Total Films: \", print_num_of_films(script_df))\n",
    "print(\"\\tNull Dataset sizes: \")\n",
    "i=0\n",
    "for key, value in db_list.items():\n",
    "    null_dist[key] = calculate_null_data(value,feature_item[i])\n",
    "    print(f'{key} : {null_dist[key]}')\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_df = plots_df[plots_df['summary'] != 'None']\n",
    "locations_df = locations_df[locations_df['locations'] != 'None']\n",
    "base_df = base_df[base_df['detail_result'] != 'None']\n",
    "box_df = box_df[box_df['detail_result'] != 'None']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660efc93",
   "metadata": {},
   "source": [
    "### Read title.basic datafile and create a base feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f79091",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_film_list = script_df.tconst.unique().tolist()\n",
    "film_list = set(original_film_list) - set(unwanted_movies)\n",
    "film_list = set(film_list) - set(['tt6422744'])\n",
    "\n",
    "imdb_data = df4[df4['tconst'].isin(film_list)]\n",
    "imdb_data.rename(columns={'tconst': 'film_id'}, inplace = True)\n",
    "print(\"title.basics: \",len(imdb_data.film_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ece89",
   "metadata": {},
   "source": [
    "### Expand Base Details and merge in base feature file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba7ccf",
   "metadata": {},
   "source": [
    "#### Include country/countries of Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_df = base_df[((base_df[\"detail_item\"] == \"Country of origin\") | \n",
    "                    (base_df[\"detail_item\"] == \"Countries of origin\")) & (base_df['film_id'].isin(film_list))].reset_index()\n",
    "origin_df.head()\n",
    "\n",
    "filter_data = origin_df.groupby(['film_id', 'detail_item']).agg({\"detail_result\": ['count',  ', '.join]}).reset_index()\n",
    "filter_data.columns = [col[0] if col[-1]=='' else col[-1] for col in filter_data.columns.values]\n",
    "filter_data.rename(columns={'join': 'countryOfOrigin', 'count': 'countryOriginCount'}, inplace = True)\n",
    "filter_data = filter_data.drop(['detail_item'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(filter_data, imdb_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['countryOriginCount'] = np.where(new_data['countryOriginCount'].isnull(), 0.0, new_data['countryOriginCount'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0521d",
   "metadata": {},
   "source": [
    "#### Include release Date and release Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "# create a translate table\n",
    "replace_table = {\n",
    "    ord('('):f'',\n",
    "    ord(')'):f''\n",
    "}\n",
    "\n",
    "# filter data for only release date item\n",
    "base_details_df = base_df[(base_df[\"detail_item\"] == \"Release date\") & (base_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "base_details_df.rename(columns={'detail_result': 'releaseDate'}, inplace=True)\n",
    "base_details_df['releaseLocation'] = base_details_df['releaseDate'].apply(lambda x: x[x.find(\"(\")+1 : x.find(\")\")])\n",
    "base_details_df['releaseLocation'] = base_details_df['releaseLocation'].apply(lambda x: x.translate(replace_table))\n",
    "base_details_df['releaseDate'] = base_details_df['releaseDate'].apply(lambda x: re.sub(r'\\(.*','',x))\n",
    "\n",
    "base_details_df[\"releaseDate\"] = pd.to_datetime(base_details_df[\"releaseDate\"], format='%B %d, %Y ', errors='ignore').astype('datetime64[ns]')\n",
    "\n",
    "# create another column for Year\n",
    "base_details_df['releaseYear'] = base_details_df[\"releaseDate\"].dt.year\n",
    "\n",
    "base_details_df[\"releaseDate\"] = pd.to_datetime(base_details_df[\"releaseDate\"], format='%Y ', errors='ignore').astype('datetime64[ns]')\n",
    "base_details_df = base_details_df.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "# merge data into film_exploration file\n",
    "new_data = pd.merge(base_details_df, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66b831",
   "metadata": {},
   "source": [
    "#### Include language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c285952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_language_count(x):\n",
    "    x_list = x.languages.split(\",\")\n",
    "    if 'None' in x_list:\n",
    "        x.languageCount -= 1\n",
    "        return(x.languageCount)\n",
    "    else:\n",
    "        return x.languageCount\n",
    "        \n",
    "lang_df = base_df[((base_df[\"detail_item\"] == \"Language\") | \n",
    "                    (base_df[\"detail_item\"] == \"Languages\")) & (base_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "lang_df['detail_result'] = np.where(lang_df['detail_result'].isnull(), 'None', lang_df['detail_result'])\n",
    "lang_data = lang_df.groupby(['film_id', 'detail_item']).agg({\"detail_result\": ['count',  ', '.join]}).reset_index()\n",
    "lang_data.columns = [col[0] if col[-1]=='' else col[-1] for col in lang_data.columns.values]\n",
    "lang_data.rename(columns={'join': 'languages', 'count': 'languageCount'}, inplace = True)\n",
    "lang_data['languageCount'] = lang_data.apply(lambda x: update_language_count(x), axis=1)\n",
    "lang_data = lang_data.drop(['detail_item'], axis=1)\n",
    "\n",
    "# merge data into film_exploration file\n",
    "new_data = pd.merge(lang_data, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['languageCount'] = np.where(new_data['languageCount'].isnull(), 0.0, new_data['languageCount'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f62df",
   "metadata": {},
   "source": [
    "#### Include production companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc680463",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_com_df = base_df[((base_df[\"detail_item\"] == \"Production company\") | \n",
    "                    (base_df[\"detail_item\"] == \"Production companies\")) & (base_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "prod_data = prod_com_df.groupby(['film_id', 'detail_item']).agg({\"detail_result\": ['count',  ', '.join]}).reset_index()\n",
    "prod_data.columns = [col[0] if col[-1]=='' else col[-1] for col in prod_data.columns.values]\n",
    "prod_data.rename(columns={'join': 'productionCompanies', 'count': 'productionCompanyCount'}, inplace = True)\n",
    "prod_data = prod_data.drop(['detail_item'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(prod_data, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['productionCompanyCount'] = np.where(new_data['productionCompanyCount'].isnull(), 0.0, new_data['productionCompanyCount'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c45e2",
   "metadata": {},
   "source": [
    "#### Include official sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df = base_df[((base_df[\"detail_item\"] == \"Official site\") | \n",
    "                    (base_df[\"detail_item\"] == \"Official sites\")) & (base_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "site_data = site_df.groupby(['film_id', 'detail_item']).agg({\"detail_result\": ['count',  ', '.join]}).reset_index()\n",
    "site_data.columns = [col[0] if col[-1]=='' else col[-1] for col in site_data.columns.values]\n",
    "site_data.rename(columns={'join': 'officialSites', 'count': 'officialSitesCount'}, inplace = True)\n",
    "site_data = site_data.drop(['detail_item'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(site_data, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['officialSitesCount'] = np.where(new_data['officialSitesCount'].isnull(), 0.0, new_data['officialSitesCount'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a25d3",
   "metadata": {},
   "source": [
    "### Expand Box Office and merge in base feature file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6a120",
   "metadata": {},
   "source": [
    "#### Include box office - Gross worldwide details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gww_df = box_df[(box_df[\"detail_item\"] == \"Gross worldwide\") & (box_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "gww_df.rename(columns={'detail_result': 'GrossWorldwide'}, inplace = True)\n",
    "gww_df = gww_df.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(gww_df, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['GrossWorldwide'] = np.where(new_data['GrossWorldwide'].isnull(), 0, new_data['GrossWorldwide'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5d1e4",
   "metadata": {},
   "source": [
    "#### Include box office - Gross US & Canada details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fed5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grossUS_df = box_df[(box_df[\"detail_item\"] == \"Gross US & Canada\") & (box_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "grossUS_df.rename(columns={'detail_result': 'GrossUSCanada'}, inplace = True)\n",
    "grossUS_df = grossUS_df.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(grossUS_df, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['GrossUSCanada'] = np.where(new_data['GrossUSCanada'].isnull(), 0, new_data['GrossUSCanada'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9884531",
   "metadata": {},
   "source": [
    "#### Include box office - Budget details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_df = box_df[(box_df[\"detail_item\"] == \"Budget\") & (box_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "budget_df.rename(columns={'detail_result': 'budget'}, inplace = True)\n",
    "budget_df = budget_df.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(budget_df, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['budget'] = np.where(new_data['budget'].isnull(), 0, new_data['budget'])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f7f08",
   "metadata": {},
   "source": [
    "#### Include box office - Opening weekend US & Canada details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "openweek_df = box_df[(box_df[\"detail_item\"] == \"Opening weekend US & Canada\") & (box_df['film_id'].isin(film_list))].reset_index()\n",
    "\n",
    "weekend_earning = openweek_df.iloc[::2]\n",
    "df_weekend = openweek_df.iloc[1::2]\n",
    "\n",
    "weekend_earning.rename(columns={'detail_result': 'weekendEarning'}, inplace = True)\n",
    "weekend_earning = weekend_earning.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "df_weekend.rename(columns={'detail_result': 'openingWeekend'}, inplace = True)\n",
    "df_weekend = df_weekend.drop(['detail_item','index'], axis=1)\n",
    "\n",
    "# merge data with film_Exploration file\n",
    "new_data = pd.merge(weekend_earning, new_data, how=\"right\", on=[\"film_id\"])\n",
    "new_data['weekendEarning'] = np.where(new_data['weekendEarning'].isnull(), 0, new_data['weekendEarning'])\n",
    "new_data = pd.merge(df_weekend, new_data, how=\"right\", on=[\"film_id\"])\n",
    "\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b2988",
   "metadata": {},
   "source": [
    "#### Convert budget values to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_data))\n",
    "print(\"null budget: \",len(new_data[new_data['budget'] == '0']))\n",
    "print(len(new_data[new_data['budget'] != '0']))\n",
    "print(\"null GrossUSCanada: \",len(new_data[new_data['GrossUSCanada'] == '0']))\n",
    "print(len(new_data[new_data['GrossUSCanada'] != '0']))\n",
    "print(\"null GrossWorldwide: \",len(new_data[new_data['GrossWorldwide'] == '0']))\n",
    "print(len(new_data[new_data['GrossWorldwide'] != '0']))\n",
    "print(\"null weekendEarning: \",len(new_data[new_data['weekendEarning'] == '0']))\n",
    "print(len(new_data[new_data['weekendEarning'] != '0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "def extract_money_value(text_data):\n",
    "    if text_data != '0':\n",
    "        # Define the regex pattern to match currency symbols followed by digits, commas, and periods until a space is found\n",
    "        pattern = r'[\\$\\¥€£]?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?'\n",
    "        match = re.findall(pattern, text_data)\n",
    "        if match:\n",
    "            value = float(sub(r'[^\\d.]', '', match[0]))\n",
    "        else:\n",
    "            print(text_data)\n",
    "            value=float(0)\n",
    "        return value\n",
    "    else:\n",
    "        return float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ca7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['budget_est'] = new_data['budget'].progress_apply(lambda x: extract_money_value(str(x)))\n",
    "new_data['GrossUSCanada'] = new_data['GrossUSCanada'].progress_apply(lambda x: extract_money_value(str(x)))\n",
    "new_data['GrossWorldwide'] = new_data['GrossWorldwide'].progress_apply(lambda x: extract_money_value(str(x)))\n",
    "new_data['weekendEarning'] = new_data['weekendEarning'].progress_apply(lambda x: extract_money_value(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e028fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_data))\n",
    "print(\"null budget: \",len(new_data[new_data['budget_est'] == 0]))\n",
    "print(len(new_data[new_data['budget_est'] != 0]))\n",
    "print(\"null GrossUSCanada: \",len(new_data[new_data['GrossUSCanada'] == 0]))\n",
    "print(len(new_data[new_data['GrossUSCanada'] != 0]))\n",
    "print(\"null GrossWorldwide: \",len(new_data[new_data['GrossWorldwide'] == 0]))\n",
    "print(len(new_data[new_data['GrossWorldwide'] != 0]))\n",
    "print(\"null weekendEarning: \",len(new_data[new_data['weekendEarning'] == 0]))\n",
    "print(len(new_data[new_data['weekendEarning'] != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e461c1",
   "metadata": {},
   "source": [
    "#### Create Budget Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ca3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def budget_label(x):\n",
    "    if x >= 1 and x < 1000000:\n",
    "        label_x = '< 1M'\n",
    "    elif x >= 1000000 and x < 2000000: \n",
    "        label_x = '1M - 2M'\n",
    "    elif x >= 2000000 and x < 5000000: \n",
    "        label_x = '2M - 5M'\n",
    "    elif x >= 5000000 and x < 7000000: \n",
    "        label_x =  '5M - 7M'\n",
    "    elif x >= 7000000 and x < 10000000: \n",
    "        label_x =  '7M - 10M'\n",
    "    elif x >= 10000000 and x < 50000000:\n",
    "        label_x =  '10M - 50M'\n",
    "    elif x >= 50000000 and x < 100000000:\n",
    "        label_x =  '50M - 100M'\n",
    "    elif x >= 100000000 and x < 500000000:\n",
    "        label_x =  '100M - 500M'\n",
    "    elif x >= 500000000 and x < 1000000000:\n",
    "        label_x =  '500M - 1B'\n",
    "    elif x >= 1000000000 and x < 10000000000:\n",
    "        label_x =  '1B - 10B'\n",
    "    elif x >= 10000000000 and x < 100000000000: \n",
    "        label_x =  '10B - 100B'\n",
    "    elif x >= 100000000000 and x < 200000000000:\n",
    "        label_x = '100B - 200B'\n",
    "    elif x >= 200000000000:\n",
    "        label_x = '>200B'\n",
    "    elif x == 0:\n",
    "        label_x = None\n",
    "\n",
    "    return label_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['budget_label'] = new_data['budget_est'].progress_apply(lambda x: budget_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e5af2b",
   "metadata": {},
   "source": [
    "#### Split multiple CountryOfOrigin and explode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e68441",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['countryOfOrigin'] = np.where(new_data['countryOfOrigin'].isnull(), 'None', new_data['countryOfOrigin'])\n",
    "new_data['coo_list'] = new_data['countryOfOrigin'].apply(lambda x: x.split(\",\"))\n",
    "new_data['coo_list'] = new_data['coo_list'].apply(lambda x: [elt.strip() for elt in x])\n",
    "new_data = new_data.explode('coo_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d388461",
   "metadata": {},
   "source": [
    "#### Split genre and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ce7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['genres'] = np.where(new_data['genres'].isnull(), 'None', new_data['genres'])\n",
    "new_data['genre_list'] = new_data['genres'].apply(lambda x: x.split(\",\"))\n",
    "new_data['genre_list'] = new_data['genre_list'].apply(lambda x: [elt.strip() for elt in x])\n",
    "new_data = new_data.explode('genre_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff86399",
   "metadata": {},
   "source": [
    "#### Create continent for CountryOfOrigin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['coo_list'] = np.where(new_data['coo_list']=='West Germany', 'Germany', new_data['coo_list'])\n",
    "new_data['coo_list'] = np.where(new_data['coo_list']=='East Germany', 'Germany', new_data['coo_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdde0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "\n",
    "country_list_europe = ['Czechoslovakia','Soviet Union','Yugoslavia','Netherlands Antilles','Federal Republic of Yugoslavia','Serbia and Montenegro',\n",
    "                      'Kosovo','Vatican']\n",
    "country_list_asia = ['Occupied Palestinian Territory','North Vietnam','Burma','Cocos Islands']\n",
    "country_list_africa = ['Reunion','The Democratic Republic of Congo']\n",
    "def extract_continent_from_country(x):\n",
    "    if x in country_list_europe:\n",
    "        return 'Europe'\n",
    "    elif x in country_list_asia:\n",
    "        return 'Asia'\n",
    "    elif x in country_list_africa:\n",
    "        return 'Africa'        \n",
    "    elif x == 'None':\n",
    "        return 'None'\n",
    "    elif x == 'Antarctica':\n",
    "        return 'Antarctica'\n",
    "    else:\n",
    "        try:\n",
    "            country_code = pc.country_name_to_country_alpha2(x, cn_name_format=\"default\")\n",
    "            continent_name = pc.convert_continent_code_to_continent_name(pc.country_alpha2_to_continent_code(country_code))\n",
    "        except:\n",
    "            print(x)\n",
    "        return continent_name\n",
    "\n",
    "new_data['coo_continent'] = new_data['coo_list'].progress_apply(lambda x: extract_continent_from_country(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bae535",
   "metadata": {},
   "source": [
    "#### Binning data into decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Release date column and fill NULL values with startYear\n",
    "\n",
    "new_data['releaseYear'] = np.where(new_data['releaseYear'].isna(), new_data['startYear'], new_data['releaseYear'])\n",
    "new_data[\"releaseYear\"] = pd.to_datetime(new_data[\"releaseYear\"]).dt.year\n",
    "new_data[new_data['releaseYear'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(new_data.releaseYear.min()-1, new_data.releaseYear.max(), 10).tolist()\n",
    "new_data['decades'] = pd.cut(x=new_data['releaseYear'], bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4112d",
   "metadata": {},
   "source": [
    "#### create Film Era bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c644833",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [\"Silent Era\", \"Transition to Sound\", \"Golden Age of Hollywood\", \"New Hollywood Era\", \"Blockbuster Era\", \"Streaming Era\"]\n",
    "new_data['film_era'] = pd.cut(new_data['releaseYear'], [1894, 1920, 1930, 1960, 1980, 2000, 2024], labels=eras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e0b8e",
   "metadata": {},
   "source": [
    "### Expand and refine Location file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b24633",
   "metadata": {},
   "source": [
    "#### Separate scene from locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ab305",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df['scene'] = locations_df['locations'].apply(lambda x: x[x.find(\"(\")+1 : x.find(\")\")] if x.find(\")\") != -1 else '\\\\N')\n",
    "locations_df['locations'] = locations_df['locations'].apply(lambda x: re.sub(r'\\(.*','',x))\n",
    "locations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1323b5",
   "metadata": {},
   "source": [
    "#### Create area, state, country, continent for locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_continent_name(continent_code: str) -> str:\n",
    "    continent_dict = {\n",
    "        \"NA\": \"North America\",\n",
    "        \"SA\": \"South America\",\n",
    "        \"AS\": \"Asia\",\n",
    "        \"AF\": \"Africa\",\n",
    "        \"OC\": \"Oceania\",\n",
    "        \"EU\": \"Europe\",\n",
    "        \"AQ\" : \"Antarctica\"\n",
    "    }\n",
    "    return continent_dict[continent_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.geocoders import Photon\n",
    "from urllib.request import Request\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import random\n",
    "\n",
    "def get_random_user_agent():\n",
    "    agents = ['Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "              'Mozilla/5.0 (Windows NT 6.3; Win64; x64; Trident/7.0; MAARJS; rv:11.0) like Gecko']\n",
    "    return random.choice(agents)\n",
    "    \n",
    "def parse_country(x, recursion=0):\n",
    "    x_list = x.split(\", \")\n",
    "    if x_list[-1] == 'USA':\n",
    "        x_list[-1] = 'United States'\n",
    "    elif x_list[-1] == 'UK':\n",
    "        x_list[-1] = 'United Kingdom'\n",
    "    x = ', '.join(x_list)\n",
    "    geolocator = Photon(user_agent=\"measurements\")\n",
    "    #add_list = x.split(\", \")\n",
    "    #country = add_list[-1]\n",
    "    try:\n",
    "        location = geolocator.geocode(x,timeout=None)\n",
    "        if (not location) and len(x_list) > 2:\n",
    "            x = ', '.join(x_list[-2:])\n",
    "            location = geolocator.geocode(x,timeout=None)\n",
    "    except:\n",
    "        time.sleep(1) # wait a bit\n",
    "        return parse_country(x)\n",
    "    if location:\n",
    "        #print(location.address)\n",
    "        #print(location.raw)\n",
    "        return location.address, location.raw\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def get_country(x):\n",
    "    if x.split(\", \")[-1] == 'Fiji' or x.split(\", \")[-1] =='Tokelau':\n",
    "        if len(x.split(\", \")) > 1:\n",
    "            return 'None', 'None', x.split(\", \")[-2], x.split(\", \")[-1], 'None','None', 'Oceania'\n",
    "        else:\n",
    "            return 'None', 'None', 'None', x.split(\", \")[-1], 'None','None', 'Oceania'\n",
    "    elif x.split(\", \")[-1] =='East Timor':\n",
    "        if len(x.split(\", \")) > 1:\n",
    "            return 'None', 'None', x.split(\", \")[-2], x.split(\", \")[-1], 'None','None', 'Asia'\n",
    "        else:\n",
    "            return 'None', 'None', 'None', x.split(\", \")[-1], 'None','None', 'Asia'        \n",
    "    else:\n",
    "        adress, coordinates = parse_country(x)\n",
    "        if adress:\n",
    "            prop = coordinates['properties']\n",
    "            coord = coordinates['geometry']\n",
    "            if \"city\" in prop:\n",
    "                city = prop['city']\n",
    "            elif \"county\" in prop:\n",
    "                city = prop['county']\n",
    "            else:\n",
    "                city = 'None'\n",
    "            if \"state\" in prop:\n",
    "                state = prop['state']\n",
    "            else:\n",
    "                state = 'None'\n",
    "            if \"country\" in prop:\n",
    "                country = prop['country']\n",
    "            else:\n",
    "                country = 'None'\n",
    "            if 'coordinates' in coord:\n",
    "                latLong = coord['coordinates']\n",
    "                lat = latLong[0]\n",
    "                long = latLong[1]\n",
    "            else:\n",
    "                lat = 'None'\n",
    "                long = 'None'\n",
    "    \n",
    "            if 'countrycode' in prop:\n",
    "                country_code = prop['countrycode']\n",
    "            else:\n",
    "                country_code = None\n",
    "    \n",
    "            if country != 'None':\n",
    "                continent = get_Continent(country)\n",
    "                if (not continent) and (country_code):\n",
    "                    continent = get_continent_from_code(country_code)\n",
    "            else:\n",
    "                continent = 'None'\n",
    "            return adress, city, state, country, lat, long, continent\n",
    "        else:\n",
    "            return 'None', 'None', 'None', 'None', 'None','None', 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert\n",
    "import pycountry\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "def get_continent_from_code(x):\n",
    "    if x is not None:\n",
    "        if x == 'VA':\n",
    "            return 'Europe'\n",
    "        if x == 'PN':\n",
    "            return 'Oceania'\n",
    "        else:\n",
    "            geolocator = Nominatim(user_agent='https')\n",
    "            try:\n",
    "                geocode = partial(geolocator.geocode, language=\"es\", timeout=None)\n",
    "            except:\n",
    "                time.sleep(1) # wait a bit\n",
    "                return get_Continent(x)\n",
    "            country_continent_name = pycountry_convert.country_alpha2_to_continent_code(x)\n",
    "            #country_continent_name = get_continent_name(x)\n",
    "            if country_continent_name:\n",
    "                return country_continent_name\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    \n",
    "def get_Continent(x):\n",
    "    if x != '\\\\N':\n",
    "        geolocator = Nominatim(user_agent='https')\n",
    "        try:\n",
    "            geocode = partial(geolocator.geocode, language=\"es\", timeout=None)\n",
    "        except:\n",
    "            time.sleep(1) # wait a bit\n",
    "            return get_Continent(x)\n",
    "        x = str(geocode(x, language=\"en\"))\n",
    "        x_list = x.split(\",\")\n",
    "        if len(x_list) >1:\n",
    "            xname = re.sub(r'^ ','',x_list[-1])\n",
    "        else:\n",
    "            xname = x_list[0]\n",
    "        country = pycountry.countries.get(name=xname)\n",
    "        #print(pycountry_convert.country_alpha2_to_continent_code(pycountry.countries.get(name=x).alpha_2))\n",
    "        if country is None:\n",
    "            return None\n",
    "        continent_code = pycountry_convert.country_alpha2_to_continent_code(country.alpha_2)\n",
    "        country_continent_name = get_continent_name(continent_code)\n",
    "    else:\n",
    "        #country_continent_name = 'None'\n",
    "        return None\n",
    "    return country_continent_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the comment before executing - below execution takes long time\n",
    "## Converted full file is already placed at the location\n",
    "#locations_df[['address','area','state','country','lattitude','longitude','continent']] = locations_df.progress_apply(lambda x: get_country(x.locations), axis=1).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f66404",
   "metadata": {},
   "source": [
    "#### Write to file - expanded location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the comment before executing - \n",
    "#locations_df.to_csv(os.path.join(os.path.dirname(__name__), \"ProcessedDataFiles/film_loc_with_continent.tsv\"), sep='\\t', index=False)\n",
    "\n",
    "filename = os.path.join(os.path.dirname(__name__), \"ProcessedDataFiles/film_loc_with_continent.tsv\")\n",
    "expand_locations_df = pd.read_csv(filename, delimiter='\\t',low_memory=False)\n",
    "expand_locations_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d653f",
   "metadata": {},
   "source": [
    "### merge locations to base feature file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_data = expand_locations_df[['film_id','country','continent']]\n",
    "locations_data.drop_duplicates(inplace=True)\n",
    "new_data = pd.merge(new_data, locations_data, how=\"left\", on=[\"film_id\"])\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef22b0",
   "metadata": {},
   "source": [
    "### Write to base feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91078b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv(os.path.join(os.path.dirname(__name__), \"ProcessedDataFiles/film_exploration_sample.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3ec68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
